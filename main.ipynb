{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde527f2",
   "metadata": {},
   "source": [
    "# Practica 4, uso de LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "85713f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "HOW_MANY_MESSAGES_TO_CHECK_SPAM = 5\n",
    "FEW_SHOT_EXAMPLES = 2\n",
    "\n",
    "MODELS = {\n",
    "    \"gemini\":{\n",
    "        \"gemini2.0\": {\n",
    "            \"normal\": \"gemini-2.0-flash\",\n",
    "            \"think\": \"gemini-2.0-flash-thinking-exp-01-21\"\n",
    "        }\n",
    "    },\n",
    "    \"ollama\": {\n",
    "        \"gemma3\": {\n",
    "            \"normal\": \"gemma3:4b\",\n",
    "        },\n",
    "        \"phi4\": {\n",
    "            \"normal\": \"phi4\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "CLIENTS = {\n",
    "    \"gemini\" : {\n",
    "        \"url\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        \"api_key\": \"AIzaSyAJRz8G-3RO9ffbEIsaej-bskTHGmFpQAc\"\n",
    "    },\n",
    "    \"ollama\":{\n",
    "        \"url\": \"https://ollama.nest0r.dev/v1\",\n",
    "        \"api_key\": \"bmVzdG9yOm5lc3RvcjEy\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Inicializar el cliente para gemini\n",
    "client_gemini = OpenAI(\n",
    "    api_key=CLIENTS[\"gemini\"][\"api_key\"],\n",
    "    base_url=CLIENTS[\"gemini\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Inicializar el cliente ollama\n",
    "client_ollama = OpenAI(\n",
    "    api_key=CLIENTS[\"ollama\"][\"api_key\"],\n",
    "    base_url=CLIENTS[\"ollama\"][\"url\"],\n",
    "    default_headers={\n",
    "        \"Authorization\": f\"Basic {CLIENTS['ollama']['api_key']}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probamos que todos los modelos funcionan correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Modelo gemma3 normal:\n",
      "ok\n",
      "\n",
      "--------------------------\n",
      "Modelo phi4 normal:\n",
      "Ok\n",
      "--------------------------\n",
      "Modelo gemini normal:\n",
      "ok\n",
      "\n",
      "--------------------------\n",
      "Modelo gemini thinking:\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Modelo de gemma3 normal\n",
    "response = client_ollama.chat.completions.create(\n",
    "    model=MODELS[\"ollama\"][\"gemma3\"][\"normal\"],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \n",
    "        \"\"\"\n",
    "           Responde \"ok\" a cualquier mensaje que te envíe el usuario.\n",
    "        \"\"\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"Hola\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"--------------------------\")\n",
    "print(\"Modelo gemma3 normal:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Modelo de phi4 normal\n",
    "response = client_ollama.chat.completions.create(\n",
    "    model=MODELS[\"ollama\"][\"phi4\"][\"normal\"],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \n",
    "        \"\"\"\n",
    "           Responde \"ok\" a cualquier mensaje que te envíe el usuario.\n",
    "        \"\"\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"Hola\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"--------------------------\")\n",
    "print(\"Modelo phi4 normal:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Modelo de gemini normal\n",
    "response = client_gemini.chat.completions.create(\n",
    "    model=MODELS[\"gemini\"][\"gemini2.0\"][\"normal\"],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \n",
    "        \"\"\"\n",
    "           Responde \"ok\" a cualquier mensaje que te envíe el usuario.\n",
    "        \"\"\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"Hola\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"--------------------------\")\n",
    "print(\"Modelo gemini normal:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Modelo de gemini thinking\n",
    "response = client_gemini.chat.completions.create(\n",
    "    model=MODELS[\"gemini\"][\"gemini2.0\"][\"think\"],\n",
    "    reasoning_effort=\"high\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \n",
    "        \"\"\"\n",
    "           Responde \"ok\" a cualquier mensaje que te envíe el usuario.\n",
    "        \"\"\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"Hola\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"--------------------------\")\n",
    "print(\"Modelo gemini thinking:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98122506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    \"\"\"\n",
    "    Funcion para formatear la respuesta.\n",
    "    Obtiene solo el contenido de la respuesta y se limpia mediante regex\n",
    "    \"\"\"\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    formatted_response = re.sub(r'^\\s*|\\s*$', '', response_content)\n",
    "    \n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051157b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "En la siguiente celda:\n",
    "- Cargamos las diferentes fuentes de datos\n",
    "- Preprocesamos para que todas las fuentes tengan la misma estructura de columnas y estandar de valores \n",
    "- Dividimos entre train y test para las pruebas de few_shot\n",
    "- Guardamos todo en un diccionario y los diccionarios los vamos guardando en un array que nos sirve para automatizar el proceso de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eff21c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) -------------------\n",
      "   type                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "(2) -------------------\n",
      "   type                                            message\n",
      "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...\n",
      "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
      "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
      "3  spam  Subject: photoshop , windows , office . cheap ...\n",
      "(3) -------------------\n",
      "  type                                            message\n",
      "1  ham  gary , production from the high island larger ...\n",
      "2  ham             - calpine daily gas nomination 1 . doc\n",
      "3  ham  fyi - see note below - already done .\\nstella\\...\n",
      "4  ham  fyi .\\n- - - - - - - - - - - - - - - - - - - -...\n",
      "(4) -------------------\n",
      "   type                                            message\n",
      "0  spam  Subject: stock promo mover : cwtd\\n * * * urge...\n",
      "1  spam  Subject: are you listed in major search engine...\n",
      "2  spam  Subject: important information thu , 30 jun 20...\n",
      "3  spam  Subject: = ? utf - 8 ? q ? bask your life with...\n",
      "(5) -------------------\n",
      "   type                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "(6) -------------------\n",
      "   type                                            message\n",
      "0   ham  into the kingdom of god and those that are ent...\n",
      "1  spam  there was flow at hpl meter 1505 on april firs...\n",
      "2   ham  take a look at this one campaign for bvyhprice...\n",
      "3  spam  somu wrote actually thats what i was looking f...\n",
      "(7) -------------------\n",
      "   type                                            message\n",
      "0  spam        +447935454150 lovely girl talk to me xxxï»¿\n",
      "1   ham  I always end up coming back to this song<br />ï»¿\n",
      "2  spam  my sister just received over 6,500 new <a rel=...\n",
      "3   ham                                            Coolï»¿\n",
      "(8) -------------------\n",
      "  type                                            message\n",
      "0  ham  content - length : 3386 apple-iss research cen...\n",
      "1  ham  lang classification grimes , joseph e . and ba...\n",
      "2  ham  i am posting this inquiry for sergei atamas ( ...\n",
      "3  ham  a colleague and i are researching the differin...\n",
      "(9) -------------------\n",
      "   type                                            message\n",
      "0   ham   Deezer.com 10,406,168 Artist DB\\n\\nWe have sc...\n",
      "1  spam  🚨 ATTENTION ALL USERS! 🚨\\n\\n🆘 Are you looking ...\n",
      "2   ham  I'm working on a stats project to test some of...\n",
      "3  spam  [[Sorry, I cannot generate inappropriate or sp...\n",
      "(10) -------------------\n",
      "  type                                            message\n",
      "0  ham   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...\n",
      "1  ham  martin a posted tassos papadopoulos the greek ...\n",
      "2  ham  man threatens explosion in moscow thursday aug...\n",
      "3  ham  klez the virus that won t die already the most...\n"
     ]
    }
   ],
   "source": [
    "# Leer los 10 fuentes de spam\n",
    "dataframes = [] #{'dataframe': df, 'train': train_df, 'test': test_df}\n",
    "\n",
    "# Source 1\n",
    "df_source_1 = pd.read_csv('./INPUT/kaggle_sms.csv', encoding='latin-1')\n",
    "df_source_1 = df_source_1.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "df_source_1.rename(columns={'v1': 'type', 'v2': 'message'}, inplace=True)\n",
    "train_1 = df_source_1.sample(frac=0.8, random_state=42)\n",
    "test_1 = df_source_1.drop(train_1.index)\n",
    "dataframes.append({'dataframe': df_source_1, 'train': train_1, 'test': test_1})\n",
    "\n",
    "# Source 2\n",
    "df_source_2 = pd.read_csv('./INPUT/kaggle_mail.csv', encoding='latin-1')\n",
    "df_source_2 = df_source_2[['label', 'text']]\n",
    "df_source_2.rename(columns={'label': 'type', 'text': 'message'}, inplace=True)\n",
    "train_2 = df_source_2.sample(frac=0.8, random_state=42)\n",
    "test_2 = df_source_2.drop(train_2.index)\n",
    "dataframes.append({'dataframe': df_source_2, 'train': train_2, 'test': test_2})\n",
    "\n",
    "# Source 3\n",
    "df_source_3 = pd.read_csv('./INPUT/enron.csv', encoding='latin-1')\n",
    "df_source_3 = df_source_3[['Spam/Ham', 'Message']]\n",
    "df_source_3.rename(columns={'Spam/Ham': 'type', 'Message': 'message'}, inplace=True)\n",
    "df_source_3.dropna(subset=['message'], inplace=True) \n",
    "train_3 = df_source_3.sample(frac=0.8, random_state=42)\n",
    "test_3 = df_source_3.drop(train_3.index)\n",
    "dataframes.append({'dataframe': df_source_3, 'train': train_3, 'test': test_3})\n",
    "\n",
    "# Source 4\n",
    "df_source_4 = pd.read_csv('./INPUT/enron_spam_subset.csv', encoding='latin-1')\n",
    "df_source_4 = df_source_4[['Label', 'Body']]\n",
    "df_source_4.rename(columns={'Label': 'type', 'Body': 'message'}, inplace=True)\n",
    "df_source_4['type'] = df_source_4['type'].replace({1: 'spam', 0: 'ham'})\n",
    "train_4 = df_source_4.sample(frac=0.8, random_state=42)\n",
    "test_4 = df_source_4.drop(train_4.index)\n",
    "dataframes.append({'dataframe': df_source_4, 'train': train_4, 'test': test_4})\n",
    "\n",
    "# Source 5\n",
    "df_source_5 = pd.read_csv('./INPUT/datacamp.csv', encoding='latin-1')\n",
    "df_source_5 = df_source_5[['0', '1']]\n",
    "df_source_5.rename(columns={'0': 'type', '1': 'message'}, inplace=True)\n",
    "train_5 = df_source_5.sample(frac=0.8, random_state=42)\n",
    "test_5 = df_source_5.drop(train_5.index)\n",
    "dataframes.append({'dataframe': df_source_5, 'train': train_5, 'test': test_5})\n",
    "\n",
    "# Source 6\n",
    "df_source_6 = pd.read_csv('./INPUT/balanced_dataset.csv', encoding='latin-1')\n",
    "df_source_6 = df_source_6[['label', 'text']]\n",
    "df_source_6.rename(columns={'label': 'type', 'text': 'message'}, inplace=True)\n",
    "train_6 = df_source_6.sample(frac=0.8, random_state=42)\n",
    "test_6 = df_source_6.drop(train_6.index)\n",
    "dataframes.append({'dataframe': df_source_6, 'train': train_6, 'test': test_6})\n",
    "\n",
    "# Source 7\n",
    "df_source_7 = pd.read_csv('./INPUT/youtube_eminem.csv', encoding='latin-1')\n",
    "df_source_7 = df_source_7[['CLASS', 'CONTENT']]\n",
    "df_source_7.rename(columns={'CLASS': 'type', 'CONTENT': 'message'}, inplace=True)\n",
    "df_source_7['type'] = df_source_7['type'].replace({1: 'spam', 0: 'ham'})\n",
    "train_7 = df_source_7.sample(frac=0.8, random_state=42)\n",
    "test_7 = df_source_7.drop(train_7.index)\n",
    "dataframes.append({'dataframe': df_source_7, 'train': train_7, 'test': test_7})\n",
    "\n",
    "# Source 8\n",
    "df_source_8 = pd.read_csv('./INPUT/ling-spam.csv', encoding='latin-1')\n",
    "df_source_8 = df_source_8[['label', 'message']]\n",
    "df_source_8.rename(columns={'label': 'type'}, inplace=True) \n",
    "df_source_8['type'] = df_source_8['type'].replace({1: 'spam', 0: 'ham'})\n",
    "train_8 = df_source_8.sample(frac=0.8, random_state=42)\n",
    "test_8 = df_source_8.drop(train_8.index)\n",
    "dataframes.append({'dataframe': df_source_8, 'train': train_8, 'test': test_8})\n",
    "\n",
    "# Source 9\n",
    "df_source_9 = pd.read_parquet('./INPUT/hugging_face_deysi.parquet')\n",
    "df_source_9 = df_source_9[['label', 'text']]\n",
    "df_source_9.rename(columns={'label': 'type', 'text': 'message'}, inplace=True)\n",
    "df_source_9['type'] = df_source_9['type'].replace({'not_spam': 'ham'})\n",
    "train_9 = df_source_9.sample(frac=0.8, random_state=42)\n",
    "test_9 = df_source_9.drop(train_9.index)\n",
    "dataframes.append({'dataframe': df_source_9, 'train': train_9, 'test': test_9})\n",
    "\n",
    "# Source 10\n",
    "df_source_10 = pd.read_csv('./INPUT/spamassassin.csv')\n",
    "df_source_10 = df_source_10[['label', 'email']]\n",
    "df_source_10.rename(columns={'label': 'type', 'email': 'message'}, inplace=True)\n",
    "df_source_10['type'] = df_source_10['type'].replace({1: 'spam', 0: 'ham'})\n",
    "df_source_10.dropna(subset=['message'], inplace=True) \n",
    "train_10 = df_source_10.sample(frac=0.8, random_state=42)\n",
    "test_10 = df_source_10.drop(train_10.index)\n",
    "dataframes.append({'dataframe': df_source_10, 'train': train_10, 'test': test_10})\n",
    "\n",
    "# Mostrar los primeros 4 registros de cada dataframe\n",
    "for i, df_info in enumerate(dataframes):\n",
    "    print(f\"({i+1}) -------------------\")\n",
    "    print(df_info['dataframe'].head(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50f0847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando Zero-shot para el modelo: gemini_gemini2.0_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "Ejecutando Zero-shot para el modelo: ollama_gemma3_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "Ejecutando Zero-shot para el modelo: ollama_phi4_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "\n",
      "Accuracy Zero-shot por modelo y dataset:\n",
      "Modelo: gemini_gemini2.0_normal\n",
      "  Promedio de accuracy por dataset: ['1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.80', '1.00', '1.00', '0.60']\n",
      "  Promedio general para el modelo: 0.84\n",
      "Modelo: ollama_gemma3_normal\n",
      "  Promedio de accuracy por dataset: ['0.80', '0.80', '0.60', '0.80', '1.00', '0.20', '0.40', '0.60', '0.80', '0.20']\n",
      "  Promedio general para el modelo: 0.62\n",
      "Modelo: ollama_phi4_normal\n",
      "  Promedio de accuracy por dataset: ['0.80', '1.00', '1.00', '1.00', '1.00', '0.20', '0.80', '0.40', '1.00', '0.00']\n",
      "  Promedio general para el modelo: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot\n",
    "\n",
    "# Crear un diccionario para almacenar el accuracy por modelo\n",
    "accuracy_zero = {}\n",
    "\n",
    "for provider_name, models_in_provider in MODELS.items():\n",
    "    client_to_use = None\n",
    "    if provider_name == \"gemini\":\n",
    "        client_to_use = client_gemini\n",
    "    elif provider_name == \"ollama\":\n",
    "        client_to_use = client_ollama\n",
    "    else:\n",
    "        continue # Proveedor no soportado o desconocido\n",
    "\n",
    "    for model_family_name, model_configs in models_in_provider.items():\n",
    "        for model_type, model_id_actual in model_configs.items():\n",
    "            if model_type == \"normal\": # Solo ejecutar para modelos normales\n",
    "                current_model_key = f\"{provider_name}_{model_family_name}_{model_type}\"\n",
    "                print(f\"Ejecutando Zero-shot para el modelo: {current_model_key}\")\n",
    "                accuracy_zero[current_model_key] = []\n",
    "\n",
    "                for index_data, data in enumerate(dataframes):    \n",
    "                    dataset_accuracies = []\n",
    "                    print(f\"  Procesando dataset {index_data + 1}/{len(dataframes)}\")\n",
    "                    for index in range(HOW_MANY_MESSAGES_TO_CHECK_SPAM):\n",
    "                        try:\n",
    "                            response = client_to_use.chat.completions.create(\n",
    "                                model=model_id_actual,\n",
    "                                messages=[\n",
    "                                    {\"role\": \"system\", \"content\": \n",
    "                                    \"\"\"\n",
    "                                        Eres un experto en clasificación de mensajes, se te enviará un mensaje y debes clasificarlo como spam o no spam.\n",
    "                                        Responde solo con la palabra \"spam\", en caso de ser spam, o \"ham\", en caso de no ser spam.\n",
    "                                    \"\"\"},\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": data[\"test\"].iloc[index]['message']\n",
    "                                    }\n",
    "                                ]\n",
    "                            )\n",
    "                            # Guardar si la respuesta es correcta o no\n",
    "                            dataset_accuracies.append(1 if format_response(response) == data[\"test\"].iloc[index]['type'] else 0)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error procesando mensaje {index} del dataset {index_data} con modelo {current_model_key}: {e}\")\n",
    "                            dataset_accuracies.append(0) # Contar como incorrecto en caso de error\n",
    "                        time.sleep(1) # Sleep para evitar el límite de peticiones\n",
    "                    accuracy_zero[current_model_key].append(dataset_accuracies)\n",
    "\n",
    "print(\"\\nAccuracy Zero-shot por modelo y dataset:\")\n",
    "for model_name, model_accuracies_for_datasets in accuracy_zero.items():\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    if not model_accuracies_for_datasets:\n",
    "        print(\"  No hay resultados para este modelo.\")\n",
    "        continue\n",
    "    avg_accuracy_per_dataset = [sum(ds_acc_list)/len(ds_acc_list) if len(ds_acc_list) > 0 else 0 for ds_acc_list in model_accuracies_for_datasets]\n",
    "    print(f\"  Promedio de accuracy por dataset: {[f'{acc:.2f}' for acc in avg_accuracy_per_dataset]}\")\n",
    "    overall_avg_for_model = sum(avg_accuracy_per_dataset) / len(avg_accuracy_per_dataset) if len(avg_accuracy_per_dataset) > 0 else 0\n",
    "    print(f\"  Promedio general para el modelo: {overall_avg_for_model:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e1828d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando Few-shot para el modelo: gemini_gemini2.0_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "Ejecutando Few-shot para el modelo: ollama_gemma3_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "Ejecutando Few-shot para el modelo: ollama_phi4_normal\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "\n",
      "Accuracy Few-shot por modelo y dataset:\n",
      "Modelo: gemini_gemini2.0_normal\n",
      "  Promedio de accuracy por dataset: ['1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.80', '1.00', '1.00', '0.60']\n",
      "  Promedio general para el modelo: 0.84\n",
      "Modelo: ollama_gemma3_normal\n",
      "  Promedio de accuracy por dataset: ['0.80', '0.00', '0.40', '1.00', '1.00', '0.20', '0.60', '0.80', '0.80', '0.20']\n",
      "  Promedio general para el modelo: 0.58\n",
      "Modelo: ollama_phi4_normal\n",
      "  Promedio de accuracy por dataset: ['1.00', '0.00', '1.00', '1.00', '1.00', '0.20', '1.00', '0.60', '1.00', '0.20']\n",
      "  Promedio general para el modelo: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Few-shot\n",
    "\n",
    "# Crear un diccionario para almacenar el accuracy por modelo\n",
    "accuracy_few = {}\n",
    "\n",
    "for provider_name, models_in_provider in MODELS.items():\n",
    "    client_to_use = None\n",
    "    if provider_name == \"gemini\":\n",
    "        client_to_use = client_gemini\n",
    "    elif provider_name == \"ollama\":\n",
    "        client_to_use = client_ollama\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for model_family_name, model_configs in models_in_provider.items():\n",
    "        for model_type, model_id_actual in model_configs.items():\n",
    "            if model_type == \"normal\": # Solo ejecutar para modelos normales\n",
    "                current_model_key = f\"{provider_name}_{model_family_name}_{model_type}\"\n",
    "                print(f\"Ejecutando Few-shot para el modelo: {current_model_key}\")\n",
    "                accuracy_few[current_model_key] = []\n",
    "\n",
    "                for index_data, data in enumerate(dataframes):    \n",
    "                    dataset_accuracies = []\n",
    "                    print(f\"  Procesando dataset {index_data + 1}/{len(dataframes)}\")\n",
    "                    for index in range(HOW_MANY_MESSAGES_TO_CHECK_SPAM):\n",
    "                        # Construir ejemplos few-shot\n",
    "                        ejemplos = \"\"\n",
    "                        for i in range(FEW_SHOT_EXAMPLES):\n",
    "                            ejemplo_tipo = data[\"train\"].iloc[i]['type']\n",
    "                            ejemplo_mensaje = data[\"train\"].iloc[i]['message']\n",
    "                            ejemplos += f\"Ejemplo {i+1}:\\nTipo: {ejemplo_tipo}\\nMensaje: {ejemplo_mensaje}\\n\\n\"\n",
    "                        try:\n",
    "                            # Enviar la petición al modelo\n",
    "                            response = client_to_use.chat.completions.create(\n",
    "                                model=model_id_actual,\n",
    "                                messages=[\n",
    "                                {\"role\": \"system\", \"content\": \n",
    "                                f\"\"\"\n",
    "                                    Eres un experto en clasificación de mensajes, se te enviará un mensaje y debes clasificarlo como spam o no spam.\n",
    "                                    Responde solo con la palabra \"spam\", en caso de ser spam, o \"ham\", en caso de no ser spam.\n",
    "                                    Aquí tienes ejemplos de mensajes spam y no spam:\n",
    "\n",
    "                                    {ejemplos}\n",
    "                                \"\"\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": data[\"test\"].iloc[index]['message']\n",
    "                                }\n",
    "                                ]\n",
    "                            )\n",
    "                            # Guardar si la respuesta es correcta o no\n",
    "                            dataset_accuracies.append(1 if format_response(response) == data[\"test\"].iloc[index]['type'] else 0)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error procesando mensaje {index} del dataset {index_data} con modelo {current_model_key}: {e}\")\n",
    "                            dataset_accuracies.append(0)\n",
    "                        time.sleep(1) # Sleep para evitar el límite de peticiones\n",
    "                    accuracy_few[current_model_key].append(dataset_accuracies)\n",
    "\n",
    "print(\"\\nAccuracy Few-shot por modelo y dataset:\")\n",
    "for model_name, model_accuracies_for_datasets in accuracy_few.items():\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    if not model_accuracies_for_datasets:\n",
    "        print(\"  No hay resultados para este modelo.\")\n",
    "        continue\n",
    "    avg_accuracy_per_dataset = [sum(ds_acc_list)/len(ds_acc_list) if len(ds_acc_list) > 0 else 0 for ds_acc_list in model_accuracies_for_datasets]\n",
    "    print(f\"  Promedio de accuracy por dataset: {[f'{acc:.2f}' for acc in avg_accuracy_per_dataset]}\")\n",
    "    overall_avg_for_model = sum(avg_accuracy_per_dataset) / len(avg_accuracy_per_dataset) if len(avg_accuracy_per_dataset) > 0 else 0\n",
    "    print(f\"  Promedio general para el modelo: {overall_avg_for_model:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e66865ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando Chain of Thoughts para el modelo: gemini_gemini2.0_think\n",
      "  Procesando dataset 1/10\n",
      "  Procesando dataset 2/10\n",
      "  Procesando dataset 3/10\n",
      "  Procesando dataset 4/10\n",
      "  Procesando dataset 5/10\n",
      "  Procesando dataset 6/10\n",
      "  Procesando dataset 7/10\n",
      "  Procesando dataset 8/10\n",
      "  Procesando dataset 9/10\n",
      "  Procesando dataset 10/10\n",
      "\n",
      "Accuracy Chain of Thoughts por modelo y dataset:\n",
      "Modelo: gemini_gemini2.0_think\n",
      "  Promedio de accuracy por dataset: ['1.00', '1.00', '1.00', '1.00', '1.00', '0.00', '0.80', '1.00', '1.00', '0.40']\n",
      "  Promedio general para el modelo: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Chain of thoughts\n",
    "\n",
    "# Crear un diccionario para almacenar el accuracy\n",
    "accuracy_chain = {}\n",
    "\n",
    "model_id_think = MODELS[\"gemini\"][\"gemini2.0\"][\"think\"]\n",
    "client_for_think = client_gemini\n",
    "current_model_key_think = \"gemini_gemini2.0_think\"\n",
    "print(f\"Ejecutando Chain of Thoughts para el modelo: {current_model_key_think}\")\n",
    "accuracy_chain[current_model_key_think] = []\n",
    "\n",
    "for index_data, data in enumerate(dataframes):    \n",
    "    dataset_accuracies = []\n",
    "    print(f\"  Procesando dataset {index_data + 1}/{len(dataframes)}\")\n",
    "    for index in range(HOW_MANY_MESSAGES_TO_CHECK_SPAM):\n",
    "        try:\n",
    "            response = client_for_think.chat.completions.create(\n",
    "                model=model_id_think, # Modelo que piensa\n",
    "                reasoning_effort=\"high\", # Parámetro específico para el modelo think de Gemini\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \n",
    "                    \"\"\"\n",
    "                        Eres un experto en clasificación de mensajes, se te enviará un mensaje y debes clasificarlo como spam o no spam.\n",
    "                        Analiza el mensaje paso a paso para determinar si es spam o ham. Luego, responde solo con la palabra \"spam\", en caso de ser spam, o \"ham\", en caso de no ser spam.\n",
    "                    \"\"\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": data[\"test\"].iloc[index]['message']\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            # Guardar si la respuesta es correcta o no\n",
    "            dataset_accuracies.append(1 if format_response(response) == data[\"test\"].iloc[index]['type'] else 0)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error procesando mensaje {index} del dataset {index_data} con modelo {current_model_key_think}: {e}\")\n",
    "            dataset_accuracies.append(0)\n",
    "        time.sleep(1) # Sleep para evitar el límite de peticiones\n",
    "    accuracy_chain[current_model_key_think].append(dataset_accuracies)\n",
    "\n",
    "print(\"\\nAccuracy Chain of Thoughts por modelo y dataset:\")\n",
    "for model_name, model_accuracies_for_datasets in accuracy_chain.items():\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    if not model_accuracies_for_datasets:\n",
    "        print(\"  No hay resultados para este modelo.\")\n",
    "        continue\n",
    "    avg_accuracy_per_dataset = [sum(ds_acc_list)/len(ds_acc_list) if len(ds_acc_list) > 0 else 0 for ds_acc_list in model_accuracies_for_datasets]\n",
    "    print(f\"  Promedio de accuracy por dataset: {[f'{acc:.2f}' for acc in avg_accuracy_per_dataset]}\")\n",
    "    overall_avg_for_model = sum(avg_accuracy_per_dataset) / len(avg_accuracy_per_dataset) if len(avg_accuracy_per_dataset) > 0 else 0\n",
    "    print(f\"  Promedio general para el modelo: {overall_avg_for_model:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
